{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suspicious Data Filter\n",
    "This is a simple prototype of how we can filter the data of our interest from a data dump.\n",
    "\n",
    "This sample algorithm is built to reduce the time it takes to browse entire data dump given by the forensic department. Model uses synthetic data built using drug dealers lingo found on internet.\n",
    "\n",
    "# Model 1\n",
    "- This model is based on a learning algorithm\n",
    "- Once model is trained it works very fast even on large data sets\n",
    "- Will be very accurate to filter the data\n",
    "\n",
    "\n",
    "- Need good amount of data for training\n",
    "- Need technology expert to make changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Settings\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100                                       crack and coke\n",
       "101                                        Weed moneyyyy\n",
       "102                         Loan for strippers and blow!\n",
       "103                                 DEFINITELY not drugs\n",
       "104                  healing cuts with hydrochloric acid\n",
       "105                                              deposit\n",
       "106                                    drink drank drunk\n",
       "107                               lots and lots of drugs\n",
       "108                                  Purple lean sizzurp\n",
       "109    I miss you man hopefully youll stop doing drug...\n",
       "110                              Shrooms (the drug kind)\n",
       "111                                             Not weed\n",
       "112                                         Opium hahaha\n",
       "113                                                Drank\n",
       "114                                             drank up\n",
       "115                          Summer lax and so not drugs\n",
       "116                  Cause I love you and weed/groceries\n",
       "117                         For more strippers and vodka\n",
       "118                                      Wine mom wasted\n",
       "119                                            Drugs man\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "data = pd.read_excel(\"Data.xlsx\")\n",
    "\n",
    "data.iloc[100:120, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and build model\n",
    "\n",
    "#randomize data set\n",
    "data_randomized1 = data.sample(frac=1, random_state=6)\n",
    "data_randomized2 = data.sample(frac=1, random_state=6)\n",
    "\n",
    "#split data to train and test\n",
    "#split_row = int(data.shape[0] * 0.8)\n",
    "#train = data_randomized[0:split_row].reset_index(drop=True)\n",
    "#test = data_randomized[split_row:].reset_index(drop=True)\n",
    "\n",
    "train = data_randomized1.copy()\n",
    "\n",
    "#clean data\n",
    "train['Message'] = train['Message'].str.replace('\\W', ' ', regex=True).str.strip().str.replace(' +', ' ', regex=True)\n",
    "train['Message'] = train['Message'].str.lower()\n",
    "\n",
    "#split messages on space\n",
    "train['Message'] = train['Message'].str.split()\n",
    "\n",
    "#collect words from all messages and filter unique\n",
    "vocabulary = []\n",
    "for sms in train['Message']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "#Create empty dictionary with all 0's as count for each word in each message\n",
    "word_counts_per_msg = {unique_word: [0] * len(train['Message']) for unique_word in vocabulary}\n",
    "\n",
    "#get the actual counts\n",
    "for index, msg in enumerate(train['Message']):\n",
    "    for word in msg:\n",
    "        word_counts_per_msg[word][index] += 1\n",
    "        \n",
    "#create new dataframe\n",
    "train_clean = pd.concat([train['Suspicious'], pd.DataFrame(word_counts_per_msg)], axis=1)\n",
    "\n",
    "#calculate probabilities\n",
    "p_susp = train_clean['Suspicious'].value_counts(normalize=True)['Yes']\n",
    "p_non_susp = train_clean['Suspicious'].value_counts(normalize=True)['No']\n",
    "\n",
    "#calculate counts\n",
    "n_susp = train_clean[train_clean['Suspicious'] == 'Yes'].sum(axis=1).sum()\n",
    "n_non_susp = train_clean[train_clean['Suspicious'] == 'No'].sum(axis=1).sum()\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "\n",
    "#create two empty dictionaries with probabilities of each word\n",
    "p_words_susp = {word: 0 for word in vocabulary}\n",
    "p_words_non_susp = {word: 0 for word in vocabulary}\n",
    "\n",
    "#split data into susp and non-susp\n",
    "train_susp = train_clean[train_clean['Suspicious'] == 'Yes']\n",
    "train_non_susp = train_clean[train_clean['Suspicious'] == 'No']\n",
    "\n",
    "#calculate probabilites\n",
    "for word in vocabulary:\n",
    "    #calculate total number times this word appeared in messages\n",
    "    n_word_susp = train_susp[word].sum()\n",
    "    n_word_non_susp = train_non_susp[word].sum()\n",
    "    \n",
    "    #calculate probabilites\n",
    "    p_word_susp = (n_word_susp + alpha)/(n_susp + (alpha * n_vocabulary))\n",
    "    p_word_non_susp = (n_word_non_susp + alpha)/(n_non_susp + (alpha * n_vocabulary))\n",
    "    \n",
    "    #append to dictionaries\n",
    "    p_words_susp[word] += p_word_susp\n",
    "    p_words_non_susp[word] += p_word_non_susp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that takes in a input string and classify the message\n",
    "def classify(message):\n",
    "    #clean and split the message\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().strip()\n",
    "    message = message.split()\n",
    "\n",
    "    #initiate values\n",
    "    p_susp_given_message = p_susp\n",
    "    p_non_susp_given_message = p_non_susp\n",
    "    \n",
    "    #calculate suspicious and non suspicious probabilities\n",
    "    for word in message:\n",
    "        if word in p_words_susp:\n",
    "            p_susp_given_message *= p_words_susp[word]\n",
    "        if word in p_words_non_susp:\n",
    "            p_non_susp_given_message *= p_words_non_susp[word]\n",
    "    \n",
    "    #return labels and probabilities\n",
    "    if p_non_susp_given_message > p_susp_given_message:\n",
    "        return 'suspicious'\n",
    "    elif p_non_susp_given_message < p_susp_given_message:\n",
    "        return 'not suspicious'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below code to output the result\n",
    "\n",
    "#data_randomized1['predicted'] = data_randomized1['Message'].apply(classify)\n",
    "#data_randomized1.loc[:,[\"Message\",\"predicted\"]].sort_values(\"predicted\", ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "This model is not based on any learning algorithm.\n",
    "\n",
    "- Easy to implement\n",
    "- Easy to make changes\n",
    "- Works well with small data\n",
    "\n",
    "\n",
    "- Will be slow if we have huge data and large number of key words to search\n",
    "- Manual work to input every keyword in the list of suspicious words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = [\"alc\", \"alcohol\", \"bubbly\", \"champagne\", \"drinks\", \"beer\", \"bud\", \"drank\", \"weed\", \"pills\", \"ecstasy\", \"broccoli\", \"plug\", \"codeine\", \"high\", \"buzzed\", \"stoned\", \"420\", \"smoke\", \"popper\", \"pods\", \"pod\", \"juul\", \"suorin\", \"vape\", \"vaping\", \"vape\", \"steroids\", \"steroid\", \"heroin\", \"drink\", \"drunk\", \"grass\", \"pill\", \"stuff\", \"drug\", \"drugs\", \"dealer\", \"crack\", \"substance\", \"illegal\", \"money\", \"purple\", \"meth\", \"opium\"]\n",
    "\n",
    "\n",
    "#Create a function that takes in a input string and classify the message\n",
    "def predict(message):\n",
    "    #clean and split the message\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().strip()\n",
    "    message = message.split()\n",
    "\n",
    "    #calculate suspicious or not\n",
    "    if any(i in message for i in bag_of_words):\n",
    "        return \"suspicious\"\n",
    "    else:\n",
    "        return \"not suspicious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suspicious'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'not suspicious'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'suspicious'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'suspicious'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'not suspicious'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try out new messages\n",
    "\n",
    "predict(\"weed?? hellya!!\")\n",
    "predict(\"Hello, how are you?\")\n",
    "predict(\"pills, pills, pills!!\")\n",
    "predict(\"http://www.stoned.com/where_to_get_weed\")\n",
    "predict(\"http://www.wikipedia.com/datascience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below to ouput the result\n",
    "\n",
    "#data_randomized2['predicted'] = data_randomized2['Message'].apply(predict)\n",
    "#data_randomized2['predicted'].value_counts\n",
    "#data_randomized2.loc[:, [\"Message\", \"predicted\"]].sort_values(\"predicted\", ascending=False).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
